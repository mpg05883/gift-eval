{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Running Chronos and Chronos-Bolt models on gift-eval benchmark\n",
    "\n",
    "This notebook shows how to run Chronos and Chronos-Bolt models on the gift-eval benchmark.\n",
    "\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT-EVAL` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class to load the data and run the model. If you have not already please check out the [dataset.ipynb](./dataset.ipynb) notebook to learn more about the `Dataset` class. We are going to just run the model on two datasets for brevity. But feel free to run on any dataset by changing the `short_datasets` and `med_long_datasets` variables below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Chronos package:\n",
    "``\n",
    "pip install chronos-forecasting\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "split_name = \"train_test\"\n",
    "info_path = Path(\"resources\") / split_name / \"info.csv\"\n",
    "\n",
    "df = pd.read_csv(info_path)\n",
    "\n",
    "prop_path = Path(\"notebooks\") / \"dataset_properties.json\"\n",
    "dataset_properties_map = json.load(open(prop_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chronos Predictor\n",
    "\n",
    "For foundation models, we need to implement a wrapper containing the model and use the wrapper to generate predicitons.\n",
    "\n",
    "This is just meant to be a simple wrapper to get you started, feel free to use your own custom implementation to wrap any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike_gee/miniconda3/envs/gift/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from chronos import BaseChronosPipeline, ForecastType\n",
    "from gluonts.itertools import batcher\n",
    "from gluonts.model import Forecast\n",
    "from gluonts.model.forecast import QuantileForecast, SampleForecast\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    quantile_levels: Optional[List[float]] = None\n",
    "    forecast_keys: List[str] = field(init=False)\n",
    "    statsforecast_keys: List[str] = field(init=False)\n",
    "    intervals: Optional[List[int]] = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.forecast_keys = [\"mean\"]\n",
    "        self.statsforecast_keys = [\"mean\"]\n",
    "        if self.quantile_levels is None:\n",
    "            self.intervals = None\n",
    "            return\n",
    "\n",
    "        intervals = set()\n",
    "\n",
    "        for quantile_level in self.quantile_levels:\n",
    "            interval = round(200 * (max(quantile_level, 1 - quantile_level) - 0.5))\n",
    "            intervals.add(interval)\n",
    "            side = \"hi\" if quantile_level > 0.5 else \"lo\"\n",
    "            self.forecast_keys.append(str(quantile_level))\n",
    "            self.statsforecast_keys.append(f\"{side}-{interval}\")\n",
    "\n",
    "        self.intervals = sorted(intervals)\n",
    "\n",
    "\n",
    "class ChronosPredictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        num_samples: int,\n",
    "        prediction_length: int,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.pipeline = BaseChronosPipeline.from_pretrained(\n",
    "            model_path,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.prediction_length = prediction_length\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def predict(self, test_data_input, batch_size: int = 1024) -> List[Forecast]:\n",
    "        pipeline = self.pipeline\n",
    "        predict_kwargs = (\n",
    "            {\"num_samples\": self.num_samples}\n",
    "            if pipeline.forecast_type == ForecastType.SAMPLES\n",
    "            else {}\n",
    "        )\n",
    "        while True:\n",
    "            try:\n",
    "                # Generate forecast samples\n",
    "                forecast_outputs = []\n",
    "                for batch in tqdm(batcher(test_data_input, batch_size=batch_size)):\n",
    "                    context = [torch.tensor(entry[\"target\"]) for entry in batch]\n",
    "                    forecast_outputs.append(\n",
    "                        pipeline.predict(\n",
    "                            context,\n",
    "                            prediction_length=self.prediction_length,\n",
    "                            **predict_kwargs,\n",
    "                        ).numpy()\n",
    "                    )\n",
    "                forecast_outputs = np.concatenate(forecast_outputs)\n",
    "                break\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                print(\n",
    "                    f\"OutOfMemoryError at batch_size {batch_size}, reducing to {batch_size // 2}\"\n",
    "                )\n",
    "                batch_size //= 2\n",
    "\n",
    "        # Convert forecast samples into gluonts Forecast objects\n",
    "        forecasts = []\n",
    "        for item, ts in zip(forecast_outputs, test_data_input):\n",
    "            forecast_start_date = ts[\"start\"] + len(ts[\"target\"])\n",
    "\n",
    "            if pipeline.forecast_type == ForecastType.SAMPLES:\n",
    "                forecasts.append(\n",
    "                    SampleForecast(samples=item, start_date=forecast_start_date)\n",
    "                )\n",
    "            elif pipeline.forecast_type == ForecastType.QUANTILES:\n",
    "                forecasts.append(\n",
    "                    QuantileForecast(\n",
    "                        forecast_arrays=item,\n",
    "                        forecast_keys=list(map(str, pipeline.quantiles)),\n",
    "                        start_date=forecast_start_date,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we have our predictor class, we can use it to predict on the gift-eval benchmark datasets. We will use the `evaluate_model` function to evaluate the model. This function is a helper function to evaluate the model on the test data and return the results in a dictionary. We are going to follow the naming conventions explained in the [README](../README.md) file to store the results in a csv file called `all_results.csv` under the `results/chronos` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(\n",
    "    WarningFilter(\"The mean prediction is not stored in the forecast data\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating chronos_bolt_base:   0%|          | 0/97 [00:00<?, ?dataset/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_length: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from gluonts.model import evaluate_model\n",
    "from gift_eval.data import Dataset\n",
    "\n",
    "model_name = \"chronos_bolt_base\"\n",
    "\n",
    "output_dir = Path(\"..\") / \"results\" / model_name / split_name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = \"all_results.csv\"\n",
    "\n",
    "csv_path = output_dir / output_file\n",
    "\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"model\",\n",
    "            \"eval_metrics/MSE[mean]\",\n",
    "            \"eval_metrics/MSE[0.5]\",\n",
    "            \"eval_metrics/MAE[0.5]\",\n",
    "            \"eval_metrics/MASE[0.5]\",\n",
    "            \"eval_metrics/MAPE[0.5]\",\n",
    "            \"eval_metrics/sMAPE[0.5]\",\n",
    "            \"eval_metrics/MSIS\",\n",
    "            \"eval_metrics/RMSE[mean]\",\n",
    "            \"eval_metrics/NRMSE[mean]\",\n",
    "            \"eval_metrics/ND[0.5]\",\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "            \"domain\",\n",
    "            \"num_variates\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "kwargs = {\"desc\": f\"Evaluating {model_name}\", \"total\": len(df), \"unit\": \"dataset\"}\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), **kwargs):\n",
    "    dataset = Dataset(name=row[\"name\"], term=row[\"term\"], verbose=False)\n",
    "\n",
    "    predictor = ChronosPredictor(\n",
    "        model_path=\"amazon/chronos-bolt-base\",\n",
    "        num_samples=20,\n",
    "        prediction_length=dataset.prediction_length,\n",
    "    )\n",
    "\n",
    "    res = evaluate_model(\n",
    "        predictor,\n",
    "        test_data=dataset.test_data,\n",
    "        metrics=metrics,\n",
    "        batch_size=1024,\n",
    "        axis=None,\n",
    "        mask_invalid_label=True,\n",
    "        allow_nan_forecast=False,\n",
    "        seasonality=dataset.seasonality,\n",
    "    )\n",
    "\n",
    "    with open(csv_path, \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(\n",
    "            [\n",
    "                dataset.config,\n",
    "                model_name,\n",
    "                res[\"MSE[mean]\"][0],\n",
    "                res[\"MSE[0.5]\"][0],\n",
    "                res[\"MAE[0.5]\"][0],\n",
    "                res[\"MASE[0.5]\"][0],\n",
    "                res[\"MAPE[0.5]\"][0],\n",
    "                res[\"sMAPE[0.5]\"][0],\n",
    "                res[\"MSIS\"][0],\n",
    "                res[\"RMSE[mean]\"][0],\n",
    "                res[\"NRMSE[mean]\"][0],\n",
    "                res[\"ND[0.5]\"][0],\n",
    "                res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                row[\"domain\"],\n",
    "                row[\"num_variates\"],\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Running the above cell will generate a csv file called `all_results.csv` under the `results/chronos` folder containing the results for the Chronos model on the gift-eval benchmark. We can display the csv file using the follow code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"../results/{model_name}/all_results.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretraining dataset information CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "split = \"pretrain\"\n",
    "info_path = Path(\"resources\") / split / \"info.csv\"\n",
    "df = pd.read_csv(info_path)\n",
    "\n",
    "print(f\"Reading {len(df)} {split} datasets...\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on each name-term combination in the pretraining split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"..\") / \"results\" / model_name / split\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_file_path = output_dir / \"all_results.csv\"\n",
    "\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"model\",\n",
    "            \"eval_metrics/MSE[mean]\",\n",
    "            \"eval_metrics/MSE[0.5]\",\n",
    "            \"eval_metrics/MAE[0.5]\",\n",
    "            \"eval_metrics/MASE[0.5]\",\n",
    "            \"eval_metrics/MAPE[0.5]\",\n",
    "            \"eval_metrics/sMAPE[0.5]\",\n",
    "            \"eval_metrics/MSIS\",\n",
    "            \"eval_metrics/RMSE[mean]\",\n",
    "            \"eval_metrics/NRMSE[mean]\",\n",
    "            \"eval_metrics/ND[0.5]\",\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "            \"domain\",\n",
    "            \"num_variates\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "kwargs = {\n",
    "    \"desc\": f\"Evaluting {model_name}\",\n",
    "    \"total\": len(df),\n",
    "    \"unit\": \"dataset\",\n",
    "}\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), **kwargs):\n",
    "    dataset = Dataset(name=row[\"name\"], term=row[\"term\"])\n",
    "\n",
    "    predictor = ChronosPredictor(\n",
    "        model_path=\"amazon/chronos-bolt-base\",\n",
    "        num_samples=20,\n",
    "        prediction_length=dataset.prediction_length,\n",
    "    )\n",
    "\n",
    "    res = evaluate_model(\n",
    "        predictor,\n",
    "        test_data=dataset.test_data,\n",
    "        metrics=metrics,\n",
    "        batch_size=1024,\n",
    "        axis=None,\n",
    "        mask_invalid_label=True,\n",
    "        allow_nan_forecast=False,\n",
    "        seasonality=dataset.seasonality,\n",
    "    )\n",
    "\n",
    "    with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(\n",
    "            [\n",
    "                dataset.config,\n",
    "                model_name,\n",
    "                res[\"MSE[mean]\"][0],\n",
    "                res[\"MSE[0.5]\"][0],\n",
    "                res[\"MAE[0.5]\"][0],\n",
    "                res[\"MASE[0.5]\"][0],\n",
    "                res[\"MAPE[0.5]\"][0],\n",
    "                res[\"sMAPE[0.5]\"][0],\n",
    "                res[\"MSIS\"][0],\n",
    "                res[\"RMSE[mean]\"][0],\n",
    "                res[\"NRMSE[mean]\"][0],\n",
    "                res[\"ND[0.5]\"][0],\n",
    "                res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "            ]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
