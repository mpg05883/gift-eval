{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Running TabPFN-TS on gift-eval benchmark\n",
    "\n",
    "This notebook shows how to run TabPFN-TS on the gift-eval benchmark.\n",
    "\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT-EVAL` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class to load the data and run the model. If you have not already please check out the [dataset.ipynb](./dataset.ipynb) notebook to learn more about the `Dataset` class. We are going to just run the model on two datasets for brevity. But feel free to run on any dataset by changing the `short_datasets` and `med_long_datasets` variables below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up TabPFN-TS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The predictor class of TabPFN-TS is included in `tabpfn-time-series` repository but not in the pip package. So we need to install it manually.\n",
    "\n",
    "1. Clone the TabPFN-TS repository\n",
    "2. Add the file to the Python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/PriorLabs/tabpfn-time-series.git\n",
    "# !cd tabpfn-time-series && git checkout v1.0.0\n",
    "# %pip install -r tabpfn-time-series/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added tabpfn-time-series to Python path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add both the main repository and the gift_eval subdirectory to the path\n",
    "sys.path.append(os.path.abspath(\"tabpfn-time-series\"))\n",
    "sys.path.append(os.path.abspath(\"tabpfn-time-series/gift_eval\"))\n",
    "print(\"Added tabpfn-time-series to Python path\")\n",
    "\n",
    "# Import the TabPFN time series predictor class\n",
    "# This is the main class we'll use for forecasting\n",
    "from tabpfn_ts_wrapper import TabPFNTSPredictor, TabPFNMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabPFN-TS Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TabPFN-TS offers two ways to run the model:\n",
    "1. `TabPFNMode.LOCAL`: Run the model on the local machine (requires GPU)\n",
    "2. `TabPFNMode.CLIENT`: Run the model on the cloud (via `tabpfn-client`)\n",
    "\n",
    "In this notebook, we will use `TabPFNMode.LOCAL` mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIFT_EVAL_TABPFN_MODE = TabPFNMode.LOCAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# short_datasets = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "short_datasets = \"ett1/D\"\n",
    "\n",
    "# med_long_datasets = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "# med_long_datasets = \"bizitobs_l2c/H\"\n",
    "\n",
    "# Get union of short and med_long datasets\n",
    "all_datasets = list(set(short_datasets.split() + med_long_datasets.split()))\n",
    "\n",
    "dataset_properties_map = json.load(open(\"dataset_properties.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Since `tabpfn-time-series` has implemented the predictor class, we can use it to predict on the gift-eval benchmark datasets. We will use the `evaluate_model` function to evaluate the model. This function is a helper function to evaluate the model on the test data and return the results in a dictionary. We are going to follow the naming conventions explained in the [README](../README.md) file to store the results in a csv file called `all_results.csv` under the `results/tabpfn_ts` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bizitobs_l2c/H', 'ett1/D']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(\n",
    "    WarningFilter(\"The mean prediction is not stored in the forecast data\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: bizitobs_l2c/H (1 of 2)\n",
      "Dataset size: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/gluonts/time_feature/seasonality.py:47: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  offset = pd.tseries.frequencies.to_offset(freq)\n",
      "/projects/bcqc/mgee2/gift-eval/src/gift_eval/data.py:151: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  freq = norm_freq_str(to_offset(self.freq).name)\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:207: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamp = pd.date_range(\n",
      "/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/tabpfn_time_series/features/feature_transformer.py:24: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  tsdf = pd.concat([train_tsdf, test_tsdf])\n",
      "GPU 0::   0%|          | 0/11 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tabpfn-v2-regressor-2noar4o2.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 490, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/joblib/parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/joblib/parallel.py\", line 607, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/tabpfn_time_series/tabpfn_worker.py\", line 212, in _prediction_routine_per_gpu\n    predictions = self._prediction_routine(\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/tabpfn_time_series/tabpfn_worker.py\", line 72, in _prediction_routine\n    tabpfn.fit(train_X, train_y)\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/contextlib.py\", line 81, in inner\n    return func(*args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/tabpfn_common_utils/telemetry/core/decorators.py\", line 288, in wrapper\n    return _safe_call_with_telemetry(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/tabpfn_common_utils/telemetry/core/decorators.py\", line 332, in _safe_call_with_telemetry\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/tabpfn/regressor.py\", line 797, in fit\n    byte_size, rng = self._initialize_model_variables()\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/tabpfn/regressor.py\", line 622, in _initialize_model_variables\n    return initialize_model_variables_helper(self, \"regressor\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/tabpfn/base.py\", line 526, in initialize_model_variables_helper\n    initialize_tabpfn_model(\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/tabpfn/base.py\", line 202, in initialize_tabpfn_model\n    load_model_criterion_config(\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/tabpfn/model_loading.py\", line 558, in load_model_criterion_config\n    loaded_model, criterion, architecture_config, inference_config = load_model(\n                                                                     ^^^^^^^^^^^\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/tabpfn/model_loading.py\", line 742, in load_model\n    checkpoint: dict = torch.load(path, map_location=\"cpu\", weights_only=None)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/torch/serialization.py\", line 1479, in load\n    with _open_file_like(f, \"rb\") as opened_file:\n         ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/torch/serialization.py\", line 759, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/torch/serialization.py\", line 740, in __init__\n    super().__init__(open(name, mode))\n                     ^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'tabpfn-v2-regressor-2noar4o2.ckpt'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     79\u001b[39m predictor = TabPFNTSPredictor(\n\u001b[32m     80\u001b[39m     ds_prediction_length=dataset.prediction_length,\n\u001b[32m     81\u001b[39m     ds_freq=dataset.freq,\n\u001b[32m     82\u001b[39m     tabpfn_mode=GIFT_EVAL_TABPFN_MODE,\n\u001b[32m     83\u001b[39m     context_length=\u001b[32m4096\u001b[39m,\n\u001b[32m     84\u001b[39m )\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Measure the time taken for evaluation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m res = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_invalid_label\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan_forecast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseasonality\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseason_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Append the results to the CSV file\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(csv_file_path, \u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m, newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/gift/lib/python3.11/site-packages/gluonts/model/evaluation.py:260\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, test_data, metrics, axis, batch_size, mask_invalid_label, allow_nan_forecast, seasonality)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_model\u001b[39m(\n\u001b[32m    237\u001b[39m     model: Predictor,\n\u001b[32m    238\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m     seasonality: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    246\u001b[39m ) -> pd.DataFrame:\n\u001b[32m    247\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m    Evaluate ``model`` when applied to ``test_data``, according to ``metrics``.\u001b[39;00m\n\u001b[32m    249\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m \u001b[33;03m    Return results as a Pandas ``DataFrame``.\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     forecasts = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluate_forecasts(\n\u001b[32m    263\u001b[39m         forecasts=forecasts,\n\u001b[32m    264\u001b[39m         test_data=test_data,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         seasonality=seasonality,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:56\u001b[39m, in \u001b[36mTabPFNTSPredictor.predict\u001b[39m\u001b[34m(self, test_data_input)\u001b[39m\n\u001b[32m     54\u001b[39m forecasts = []\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batcher(test_data_input, batch_size=\u001b[32m1024\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     forecasts.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m forecasts\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/gift_eval/tabpfn_ts_wrapper.py:67\u001b[39m, in \u001b[36mTabPFNTSPredictor._predict_batch\u001b[39m\u001b[34m(self, test_data_input)\u001b[39m\n\u001b[32m     64\u001b[39m train_tsdf, test_tsdf = \u001b[38;5;28mself\u001b[39m._preprocess_test_data(test_data_input)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m pred: TimeSeriesDataFrame = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtabpfn_predictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tsdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_tsdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m pred = pred.drop(columns=[\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Pre-allocate forecasts list and get forecast quantile keys\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/tabpfn_time_series/predictor.py:48\u001b[39m, in \u001b[36mTabPFNTimeSeriesPredictor.predict\u001b[39m\u001b[34m(self, train_tsdf, test_tsdf)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03mPredict on each time series individually (local forecasting).\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     44\u001b[39m logger.info(\n\u001b[32m     45\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredicting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_tsdf.item_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m time series with config\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.tabpfn_worker.config\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtabpfn_worker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tsdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_tsdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projects/bcqc/mgee2/gift-eval/tabpfn-time-series/tabpfn_time_series/tabpfn_worker.py:172\u001b[39m, in \u001b[36mLocalTabPFN.predict\u001b[39m\u001b[34m(self, train_tsdf, test_tsdf)\u001b[39m\n\u001b[32m    166\u001b[39m item_ids_chunks = np.array_split(\n\u001b[32m    167\u001b[39m     np.random.permutation(train_tsdf.item_ids),\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mmin\u001b[39m(total_num_workers, \u001b[38;5;28mlen\u001b[39m(train_tsdf.item_ids)),\n\u001b[32m    169\u001b[39m )\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# Run predictions in parallel\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m predictions = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem_ids_chunks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mloky\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prediction_routine_per_gpu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_tsdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_tsdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgpu_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Alternate between available GPUs\u001b[39;49;00m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem_ids_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m predictions = pd.concat(predictions)\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# Sort predictions according to original item_ids order\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/gift/lib/python3.11/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/gift/lib/python3.11/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/gift/lib/python3.11/site-packages/joblib/parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/gift/lib/python3.11/site-packages/joblib/parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/gift/lib/python3.11/site-packages/joblib/parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/gift/lib/python3.11/site-packages/joblib/parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'tabpfn-v2-regressor-2noar4o2.ckpt'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "from src.gift_eval.data import Dataset\n",
    "\n",
    "# Iterate over all available datasets\n",
    "model_name = \"tabpfn_ts\"\n",
    "output_dir = f\"../results/{model_name}\"\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")\n",
    "\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"model\",\n",
    "            \"eval_metrics/MSE[mean]\",\n",
    "            \"eval_metrics/MSE[0.5]\",\n",
    "            \"eval_metrics/MAE[0.5]\",\n",
    "            \"eval_metrics/MASE[0.5]\",\n",
    "            \"eval_metrics/MAPE[0.5]\",\n",
    "            \"eval_metrics/sMAPE[0.5]\",\n",
    "            \"eval_metrics/MSIS\",\n",
    "            \"eval_metrics/RMSE[mean]\",\n",
    "            \"eval_metrics/NRMSE[mean]\",\n",
    "            \"eval_metrics/ND[0.5]\",\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "            \"domain\",\n",
    "            \"num_variates\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "    ds_key = ds_name.split(\"/\")[0]\n",
    "    print(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (\n",
    "            term == \"medium\" or term == \"long\"\n",
    "        ) and ds_name not in med_long_datasets.split():\n",
    "            continue\n",
    "\n",
    "        if \"/\" in ds_name:\n",
    "            ds_key = ds_name.split(\"/\")[0]\n",
    "            ds_freq = ds_name.split(\"/\")[1]\n",
    "            ds_key = ds_key.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "        else:\n",
    "            ds_key = ds_name.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "        ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "        # Initialize the dataset\n",
    "        to_univariate = (\n",
    "            False\n",
    "            if Dataset(name=ds_name, term=term, to_univariate=False).target_dim == 1\n",
    "            else True\n",
    "        )\n",
    "        dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "        season_length = get_seasonality(dataset.freq)\n",
    "        print(f\"Dataset size: {len(dataset.test_data)}\")\n",
    "        predictor = TabPFNTSPredictor(\n",
    "            ds_prediction_length=dataset.prediction_length,\n",
    "            ds_freq=dataset.freq,\n",
    "            tabpfn_mode=GIFT_EVAL_TABPFN_MODE,\n",
    "            context_length=4096,\n",
    "        )\n",
    "        # Measure the time taken for evaluation\n",
    "        res = evaluate_model(\n",
    "            predictor,\n",
    "            test_data=dataset.test_data,\n",
    "            metrics=metrics,\n",
    "            batch_size=1024,\n",
    "            axis=None,\n",
    "            mask_invalid_label=True,\n",
    "            allow_nan_forecast=False,\n",
    "            seasonality=season_length,\n",
    "        )\n",
    "\n",
    "        # Append the results to the CSV file\n",
    "        with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    ds_config,\n",
    "                    model_name,\n",
    "                    res[\"MSE[mean]\"][0],\n",
    "                    res[\"MSE[0.5]\"][0],\n",
    "                    res[\"MAE[0.5]\"][0],\n",
    "                    res[\"MASE[0.5]\"][0],\n",
    "                    res[\"MAPE[0.5]\"][0],\n",
    "                    res[\"sMAPE[0.5]\"][0],\n",
    "                    res[\"MSIS\"][0],\n",
    "                    res[\"RMSE[mean]\"][0],\n",
    "                    res[\"NRMSE[mean]\"][0],\n",
    "                    res[\"ND[0.5]\"][0],\n",
    "                    res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                    dataset_properties_map[ds_key][\"domain\"],\n",
    "                    dataset_properties_map[ds_key][\"num_variates\"],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        print(f\"Results for {ds_name} have been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Running the above cell will generate a csv file called `all_results.csv` under the `results/tabpfn_ts` folder containing the results for the Chronos model on the gift-eval benchmark. We can display the csv file using the follow code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"../results/{model_name}/all_results.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gift",
   "language": "python",
   "name": "gift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
