{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Running Chronos and Chronos-Bolt models on gift-eval benchmark\n",
    "\n",
    "This notebook shows how to run Chronos and Chronos-Bolt models on the gift-eval benchmark.\n",
    "\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT-EVAL` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class to load the data and run the model. If you have not already please check out the [dataset.ipynb](./dataset.ipynb) notebook to learn more about the `Dataset` class. We are going to just run the model on two datasets for brevity. But feel free to run on any dataset by changing the `short_datasets` and `med_long_datasets` variables below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Chronos package:\n",
    "``\n",
    "pip install chronos-forecasting\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "short_datasets = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "med_long_datasets = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "\n",
    "# Get union of short and med_long datasets\n",
    "all_datasets = list(set(short_datasets.split() + med_long_datasets.split()))\n",
    "\n",
    "dataset_properties_map = json.load(open(\"dataset_properties.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chronos Predictor\n",
    "\n",
    "For foundation models, we need to implement a wrapper containing the model and use the wrapper to generate predicitons.\n",
    "\n",
    "This is just meant to be a simple wrapper to get you started, feel free to use your own custom implementation to wrap any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from chronos import BaseChronosPipeline, ForecastType\n",
    "from gluonts.itertools import batcher\n",
    "from gluonts.model import Forecast\n",
    "from gluonts.model.forecast import QuantileForecast, SampleForecast\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    quantile_levels: Optional[List[float]] = None\n",
    "    forecast_keys: List[str] = field(init=False)\n",
    "    statsforecast_keys: List[str] = field(init=False)\n",
    "    intervals: Optional[List[int]] = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.forecast_keys = [\"mean\"]\n",
    "        self.statsforecast_keys = [\"mean\"]\n",
    "        if self.quantile_levels is None:\n",
    "            self.intervals = None\n",
    "            return\n",
    "\n",
    "        intervals = set()\n",
    "\n",
    "        for quantile_level in self.quantile_levels:\n",
    "            interval = round(200 * (max(quantile_level, 1 - quantile_level) - 0.5))\n",
    "            intervals.add(interval)\n",
    "            side = \"hi\" if quantile_level > 0.5 else \"lo\"\n",
    "            self.forecast_keys.append(str(quantile_level))\n",
    "            self.statsforecast_keys.append(f\"{side}-{interval}\")\n",
    "\n",
    "        self.intervals = sorted(intervals)\n",
    "\n",
    "\n",
    "class ChronosPredictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        num_samples: int,\n",
    "        prediction_length: int,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        print(\"prediction_length:\", prediction_length)\n",
    "        self.pipeline = BaseChronosPipeline.from_pretrained(\n",
    "            model_path,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.prediction_length = prediction_length\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def predict(self, test_data_input, batch_size: int = 1024) -> List[Forecast]:\n",
    "        pipeline = self.pipeline\n",
    "        predict_kwargs = (\n",
    "            {\"num_samples\": self.num_samples}\n",
    "            if pipeline.forecast_type == ForecastType.SAMPLES\n",
    "            else {}\n",
    "        )\n",
    "        while True:\n",
    "            try:\n",
    "                # Generate forecast samples\n",
    "                forecast_outputs = []\n",
    "                for batch in tqdm(batcher(test_data_input, batch_size=batch_size)):\n",
    "                    context = [torch.tensor(entry[\"target\"]) for entry in batch]\n",
    "                    forecast_outputs.append(\n",
    "                        pipeline.predict(\n",
    "                            context,\n",
    "                            prediction_length=self.prediction_length,\n",
    "                            **predict_kwargs,\n",
    "                        ).numpy()\n",
    "                    )\n",
    "                forecast_outputs = np.concatenate(forecast_outputs)\n",
    "                break\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                print(\n",
    "                    f\"OutOfMemoryError at batch_size {batch_size}, reducing to {batch_size // 2}\"\n",
    "                )\n",
    "                batch_size //= 2\n",
    "\n",
    "        # Convert forecast samples into gluonts Forecast objects\n",
    "        forecasts = []\n",
    "        for item, ts in zip(forecast_outputs, test_data_input):\n",
    "            forecast_start_date = ts[\"start\"] + len(ts[\"target\"])\n",
    "\n",
    "            if pipeline.forecast_type == ForecastType.SAMPLES:\n",
    "                forecasts.append(\n",
    "                    SampleForecast(samples=item, start_date=forecast_start_date)\n",
    "                )\n",
    "            elif pipeline.forecast_type == ForecastType.QUANTILES:\n",
    "                forecasts.append(\n",
    "                    QuantileForecast(\n",
    "                        forecast_arrays=item,\n",
    "                        forecast_keys=list(map(str, pipeline.quantiles)),\n",
    "                        start_date=forecast_start_date,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we have our predictor class, we can use it to predict on the gift-eval benchmark datasets. We will use the `evaluate_model` function to evaluate the model. This function is a helper function to evaluate the model on the test data and return the results in a dictionary. We are going to follow the naming conventions explained in the [README](../README.md) file to store the results in a csv file called `all_results.csv` under the `results/chronos` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bizitobs_application',\n",
       " 'kdd_cup_2018_with_missing/H',\n",
       " 'M_DENSE/D',\n",
       " 'ett2/W',\n",
       " 'electricity/H',\n",
       " 'solar/10T',\n",
       " 'ett1/15T',\n",
       " 'm4_monthly',\n",
       " 'temperature_rain_with_missing',\n",
       " 'hierarchical_sales/W',\n",
       " 'ett1/D',\n",
       " 'SZ_TAXI/H',\n",
       " 'jena_weather/D',\n",
       " 'jena_weather/H',\n",
       " 'm4_weekly',\n",
       " 'hospital',\n",
       " 'car_parts_with_missing',\n",
       " 'M_DENSE/H',\n",
       " 'ett1/W',\n",
       " 'LOOP_SEATTLE/H',\n",
       " 'SZ_TAXI/15T',\n",
       " 'bitbrains_fast_storage/5T',\n",
       " 'm4_hourly',\n",
       " 'LOOP_SEATTLE/D',\n",
       " 'm4_daily',\n",
       " 'us_births/D',\n",
       " 'ett2/H',\n",
       " 'saugeenday/D',\n",
       " 'solar/H',\n",
       " 'm4_quarterly',\n",
       " 'us_births/M',\n",
       " 'solar/D',\n",
       " 'saugeenday/M',\n",
       " 'LOOP_SEATTLE/5T',\n",
       " 'ett2/D',\n",
       " 'jena_weather/10T',\n",
       " 'electricity/15T',\n",
       " 'bitbrains_rnd/H',\n",
       " 'bitbrains_fast_storage/H',\n",
       " 'bizitobs_service',\n",
       " 'restaurant',\n",
       " 'm4_yearly',\n",
       " 'us_births/W',\n",
       " 'saugeenday/W',\n",
       " 'bizitobs_l2c/5T',\n",
       " 'electricity/D',\n",
       " 'hierarchical_sales/D',\n",
       " 'bitbrains_rnd/5T',\n",
       " 'kdd_cup_2018_with_missing/D',\n",
       " 'solar/W',\n",
       " 'bizitobs_l2c/H',\n",
       " 'ett2/15T',\n",
       " 'ett1/H',\n",
       " 'electricity/W',\n",
       " 'covid_deaths']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(\n",
    "    WarningFilter(\"The mean prediction is not stored in the forecast data\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: bizitobs_application (1 of 55)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'notebooks/dataset_properties.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m ds_config = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_freq\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Initialize the dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m dataset = \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m season_length = get_seasonality(dataset.freq)\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset.test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gift-eval/src/gift_eval/data.py:149\u001b[39m, in \u001b[36mDataset.__init__\u001b[39m\u001b[34m(self, name, term, storage_env_var, verbose)\u001b[39m\n\u001b[32m    147\u001b[39m load_dotenv()\n\u001b[32m    148\u001b[39m directory = os.getenv(storage_env_var)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m storage_path = \u001b[38;5;28mstr\u001b[39m(Path(directory) / \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdirectory\u001b[49m / \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m    151\u001b[39m \u001b[38;5;28mself\u001b[39m.hf_dataset = datasets.load_from_disk(storage_path).with_format(\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m process = ProcessDataEntry(\n\u001b[32m    154\u001b[39m     \u001b[38;5;28mself\u001b[39m.freq,\n\u001b[32m    155\u001b[39m     one_dim_target=\u001b[38;5;28mself\u001b[39m.target_dim == \u001b[32m1\u001b[39m,\n\u001b[32m    156\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gift/lib/python3.11/functools.py:1001\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m    999\u001b[39m val = cache.get(\u001b[38;5;28mself\u001b[39m.attrname, _NOT_FOUND)\n\u001b[32m   1000\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1003\u001b[39m         cache[\u001b[38;5;28mself\u001b[39m.attrname] = val\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gift-eval/src/gift_eval/data.py:211\u001b[39m, in \u001b[36mDataset.subdirectory\u001b[39m\u001b[34m(self, file_name, directory)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msubdirectory\u001b[39m(\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    191\u001b[39m     file_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mdataset_properties.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    192\u001b[39m     directory: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mnotebooks\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    193\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    194\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[33;03m    Determines whether the storage path's subdirectory is set to \"pretrain\"\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m    or \"train_test\".\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    209\u001b[39m \u001b[33;03m            else \"train_test\".\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     dataset_properties = json.load(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtrain_test\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.key \u001b[38;5;129;01min\u001b[39;00m dataset_properties \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mpretrain\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'notebooks/dataset_properties.json'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "from gift_eval.data import Dataset\n",
    "\n",
    "# Iterate over all available datasets\n",
    "\n",
    "model_name = \"chronos_bolt_base\"\n",
    "output_dir = f\"../results/{model_name}/train_test\"\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = Path(output_dir) / \"results.csv\"\n",
    "\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"model\",\n",
    "            \"eval_metrics/MSE[mean]\",\n",
    "            \"eval_metrics/MSE[0.5]\",\n",
    "            \"eval_metrics/MAE[0.5]\",\n",
    "            \"eval_metrics/MASE[0.5]\",\n",
    "            \"eval_metrics/MAPE[0.5]\",\n",
    "            \"eval_metrics/sMAPE[0.5]\",\n",
    "            \"eval_metrics/MSIS\",\n",
    "            \"eval_metrics/RMSE[mean]\",\n",
    "            \"eval_metrics/NRMSE[mean]\",\n",
    "            \"eval_metrics/ND[0.5]\",\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "            \"domain\",\n",
    "            \"num_variates\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "    ds_key = ds_name.split(\"/\")[0]\n",
    "    print(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (\n",
    "            term == \"medium\" or term == \"long\"\n",
    "        ) and ds_name not in med_long_datasets.split():\n",
    "            continue\n",
    "\n",
    "        if \"/\" in ds_name:\n",
    "            ds_key = ds_name.split(\"/\")[0]\n",
    "            ds_freq = ds_name.split(\"/\")[1]\n",
    "            ds_key = ds_key.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "        else:\n",
    "            ds_key = ds_name.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "        ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "        # Initialize the dataset\n",
    "        dataset = Dataset(name=ds_name, term=term, verbose=True)\n",
    "        season_length = get_seasonality(dataset.freq)\n",
    "        print(f\"Dataset size: {len(dataset.test_data)}\")\n",
    "\n",
    "        predictor = ChronosPredictor(\n",
    "            # use \"amazon/chronos-t5-base\" for the corresponding original Chronos model\n",
    "            model_path=\"amazon/chronos-bolt-base\",\n",
    "            num_samples=20,\n",
    "            prediction_length=dataset.prediction_length,\n",
    "        )\n",
    "\n",
    "        # Measure the time taken for evaluation\n",
    "        res = evaluate_model(\n",
    "            predictor,\n",
    "            test_data=dataset.test_data,\n",
    "            metrics=metrics,\n",
    "            batch_size=1024,\n",
    "            axis=None,\n",
    "            mask_invalid_label=True,\n",
    "            allow_nan_forecast=False,\n",
    "            seasonality=season_length,\n",
    "        )\n",
    "\n",
    "        # Append the results to the CSV file\n",
    "        with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    ds_config,\n",
    "                    model_name,\n",
    "                    res[\"MSE[mean]\"][0],\n",
    "                    res[\"MSE[0.5]\"][0],\n",
    "                    res[\"MAE[0.5]\"][0],\n",
    "                    res[\"MASE[0.5]\"][0],\n",
    "                    res[\"MAPE[0.5]\"][0],\n",
    "                    res[\"sMAPE[0.5]\"][0],\n",
    "                    res[\"MSIS\"][0],\n",
    "                    res[\"RMSE[mean]\"][0],\n",
    "                    res[\"NRMSE[mean]\"][0],\n",
    "                    res[\"ND[0.5]\"][0],\n",
    "                    res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                    dataset_properties_map[ds_key][\"domain\"],\n",
    "                    dataset_properties_map[ds_key][\"num_variates\"],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        print(f\"Results for {ds_name} have been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Running the above cell will generate a csv file called `all_results.csv` under the `results/chronos` folder containing the results for the Chronos model on the gift-eval benchmark. We can display the csv file using the follow code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>eval_metrics/MSE[mean]</th>\n",
       "      <th>eval_metrics/MSE[0.5]</th>\n",
       "      <th>eval_metrics/MAE[0.5]</th>\n",
       "      <th>eval_metrics/MASE[0.5]</th>\n",
       "      <th>eval_metrics/MAPE[0.5]</th>\n",
       "      <th>eval_metrics/sMAPE[0.5]</th>\n",
       "      <th>eval_metrics/MSIS</th>\n",
       "      <th>eval_metrics/RMSE[mean]</th>\n",
       "      <th>eval_metrics/NRMSE[mean]</th>\n",
       "      <th>eval_metrics/ND[0.5]</th>\n",
       "      <th>eval_metrics/mean_weighted_sum_quantile_loss</th>\n",
       "      <th>domain</th>\n",
       "      <th>num_variates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bizitobs_l2c/H/short</td>\n",
       "      <td>chronos_bolt_base</td>\n",
       "      <td>56.592417</td>\n",
       "      <td>56.592417</td>\n",
       "      <td>4.475569</td>\n",
       "      <td>0.432270</td>\n",
       "      <td>0.395127</td>\n",
       "      <td>0.607594</td>\n",
       "      <td>2.824355</td>\n",
       "      <td>7.522793</td>\n",
       "      <td>0.405498</td>\n",
       "      <td>0.241245</td>\n",
       "      <td>0.189003</td>\n",
       "      <td>Web/CloudOps</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bizitobs_l2c/H/medium</td>\n",
       "      <td>chronos_bolt_base</td>\n",
       "      <td>74.003423</td>\n",
       "      <td>74.003423</td>\n",
       "      <td>4.934654</td>\n",
       "      <td>0.495206</td>\n",
       "      <td>0.499253</td>\n",
       "      <td>0.755523</td>\n",
       "      <td>6.702805</td>\n",
       "      <td>8.602524</td>\n",
       "      <td>0.520896</td>\n",
       "      <td>0.298801</td>\n",
       "      <td>0.254430</td>\n",
       "      <td>Web/CloudOps</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bizitobs_l2c/H/long</td>\n",
       "      <td>chronos_bolt_base</td>\n",
       "      <td>77.461049</td>\n",
       "      <td>77.461049</td>\n",
       "      <td>5.131364</td>\n",
       "      <td>0.555710</td>\n",
       "      <td>0.608672</td>\n",
       "      <td>0.786079</td>\n",
       "      <td>10.303878</td>\n",
       "      <td>8.801196</td>\n",
       "      <td>0.537602</td>\n",
       "      <td>0.313438</td>\n",
       "      <td>0.277636</td>\n",
       "      <td>Web/CloudOps</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m4_weekly/W/short</td>\n",
       "      <td>chronos_bolt_base</td>\n",
       "      <td>240658.128991</td>\n",
       "      <td>240658.128991</td>\n",
       "      <td>257.703530</td>\n",
       "      <td>2.077569</td>\n",
       "      <td>0.062621</td>\n",
       "      <td>0.062523</td>\n",
       "      <td>18.150360</td>\n",
       "      <td>490.569189</td>\n",
       "      <td>0.089374</td>\n",
       "      <td>0.046950</td>\n",
       "      <td>0.038168</td>\n",
       "      <td>Econ/Fin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dataset              model  eval_metrics/MSE[mean]   \n",
       "0   bizitobs_l2c/H/short  chronos_bolt_base               56.592417  \\\n",
       "1  bizitobs_l2c/H/medium  chronos_bolt_base               74.003423   \n",
       "2    bizitobs_l2c/H/long  chronos_bolt_base               77.461049   \n",
       "3      m4_weekly/W/short  chronos_bolt_base           240658.128991   \n",
       "\n",
       "   eval_metrics/MSE[0.5]  eval_metrics/MAE[0.5]  eval_metrics/MASE[0.5]   \n",
       "0              56.592417               4.475569                0.432270  \\\n",
       "1              74.003423               4.934654                0.495206   \n",
       "2              77.461049               5.131364                0.555710   \n",
       "3          240658.128991             257.703530                2.077569   \n",
       "\n",
       "   eval_metrics/MAPE[0.5]  eval_metrics/sMAPE[0.5]  eval_metrics/MSIS   \n",
       "0                0.395127                 0.607594           2.824355  \\\n",
       "1                0.499253                 0.755523           6.702805   \n",
       "2                0.608672                 0.786079          10.303878   \n",
       "3                0.062621                 0.062523          18.150360   \n",
       "\n",
       "   eval_metrics/RMSE[mean]  eval_metrics/NRMSE[mean]  eval_metrics/ND[0.5]   \n",
       "0                 7.522793                  0.405498              0.241245  \\\n",
       "1                 8.602524                  0.520896              0.298801   \n",
       "2                 8.801196                  0.537602              0.313438   \n",
       "3               490.569189                  0.089374              0.046950   \n",
       "\n",
       "   eval_metrics/mean_weighted_sum_quantile_loss        domain  num_variates  \n",
       "0                                      0.189003  Web/CloudOps             7  \n",
       "1                                      0.254430  Web/CloudOps             7  \n",
       "2                                      0.277636  Web/CloudOps             7  \n",
       "3                                      0.038168      Econ/Fin             1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"../results/{model_name}/all_results.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the names of the pretraining datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the pretraining datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
