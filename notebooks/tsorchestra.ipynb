{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart - Run TS Orchestra on GiftEval\n",
    "\n",
    "This notebook shows how to run TS Orchestra on the gift-eval benchmark.\n",
    "\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT-EVAL` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class to load the data and run the model. If you have not already please check out the [dataset.ipynb](./dataset.ipynb) notebook to learn more about the `Dataset` class. We are going to just run the model on two datasets for brevity. But feel free to run on any dataset by changing the `short_datasets` and `med_long_datasets` variables below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up TS Orchestra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the TS Orchestra repository and add the file to the python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace this url with the real TS Orchestra repository when it is made public\n",
    "# !git clone https://github.com/mpg05883/Private-TS-Orchestra.git\n",
    "\n",
    "# cd ./ts-orchestra && pip install -e .\n",
    "# pip install dotted_dict, tabulate, timecopilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added ts-orchestra to Python path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " See https://github.com/google-research/timesfm/blob/master/README.md for updated APIs.\n",
      "Imported tsorchestra from: /projects/bcqc/mgee2/gift-eval/ts-orchestra/src/tsorchestra/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the ts-orchestra subdirectory to the path\n",
    "sys.path.append(os.path.abspath(\"ts-orchestra\"))\n",
    "sys.path.append(os.path.abspath(\"ts-orchestra/src\"))\n",
    "print(\"Added ts-orchestra to Python path\")\n",
    "\n",
    "import tsorchestra\n",
    "print(f\"Imported tsorchestra from: {tsorchestra.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the datasets to evaluate TS Orchestra on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# short_datasets = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "short_datasets = \"ett1/H\"\n",
    "\n",
    "# med_long_datasets = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "med_long_datasets = \"\"\n",
    "\n",
    "# Get union of short and med_long datasets\n",
    "all_datasets = list(set(short_datasets.split() + med_long_datasets.split()))\n",
    "\n",
    "dataset_properties_map = json.load(open(\"dataset_properties.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the metrics to use during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.ev.metrics import (\n",
    "    MSE,\n",
    "    MAE,\n",
    "    MASE,\n",
    "    MAPE,\n",
    "    SMAPE,\n",
    "    MSIS,\n",
    "    RMSE,\n",
    "    NRMSE,\n",
    "    ND,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-1649965:t-139854902654784:slsqp.py:__init__:[SLSQPEnsemble] Initializing ensemble with 6 models (Chronos, FlowState, Moirai, Sundial, TimesFM, Toto), metric=mae, n_windows=1\n"
     ]
    }
   ],
   "source": [
    "from tsorchestra.models.foundation import (\n",
    "    Moirai,\n",
    "    Sundial,\n",
    "    Toto,\n",
    "    Chronos,\n",
    "    FlowState,\n",
    "    TimesFM,\n",
    ")\n",
    "from tsorchestra.models.ensembles import SLSQPEnsemble\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Metric to optimize during cross-validation\n",
    "metric = \"mae\"\n",
    "\n",
    "# Load models\n",
    "models = [\n",
    "    Moirai(batch_size=batch_size),\n",
    "    Sundial(batch_size=batch_size),\n",
    "    Toto(batch_size=batch_size),\n",
    "    Chronos(batch_size=batch_size),\n",
    "    FlowState(batch_size=batch_size),\n",
    "    TimesFM(batch_size=batch_size),\n",
    "]\n",
    "\n",
    "# Use models to create ensemble forecaster\n",
    "forecaster = SLSQPEnsemble(\n",
    "    models=models,\n",
    "    metric=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate TS Orchestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-1649965:t-139854902654784:seed.py:seed_everything:[rank: 0] Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: ett1/H (1 of 1)\n",
      "Dataset size: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GluonTSPredictor] Predicting: 100%|██████████| 140/140 [00:00<00:00, 110128.01series/s]\n",
      "[Moirai] Cross-validating:   0%|          | 0/1 [00:00<?, ?window/s]INFO:p-1649965:t-139854902654784:pandas.py:from_long_dataframe:Indexing data by 'ds'.\n",
      "INFO:p-1649965:t-139854902654784:pandas.py:from_long_dataframe:Grouping data by 'unique_id'; this may take some time.\n",
      "INFO:p-1649965:t-139854902654784:forecast_generator.py:log_once:Forecast is not sample based. Ignoring parameter `num_samples` from predict method.\n",
      "140it [00:00, 180.44it/s]\n",
      "[Moirai] Cross-validating: 100%|██████████| 1/1 [00:02<00:00,  2.15s/window]\n",
      "[Sundial] Cross-validating:   0%|          | 0/1 [00:00<?, ?window/s]/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.45it/s]\n",
      "[Sundial] Cross-validating: 100%|██████████| 1/1 [00:04<00:00,  4.73s/window]\n",
      "100%|██████████| 5/5 [00:29<00:00,  5.86s/it] [00:00<?, ?window/s]\n",
      "[Toto] Cross-validating: 100%|██████████| 1/1 [00:32<00:00, 32.56s/window]\n",
      "[Chronos] Cross-validating:   0%|          | 0/1 [00:00<?, ?window/s]/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 5/5 [00:20<00:00,  4.05s/it]\n",
      "[Chronos] Cross-validating: 100%|██████████| 1/1 [00:23<00:00, 23.04s/window]\n",
      "[FlowState] Cross-validating:   0%|          | 0/1 [00:00<?, ?window/s]/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:p-1649965:t-139854902654784:modeling_flowstate.py:__init__:Number of encoder parameters: 7885.8240000000005k\n",
      "INFO:p-1649965:t-139854902654784:modeling_flowstate.py:__init__:Number of dencoder parameters: 1181.952k (14.99%)\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.90it/s]\n",
      "[FlowState] Cross-validating: 100%|██████████| 1/1 [00:03<00:00,  3.52s/window]\n",
      "[_TimesFMV2_p5] Cross-validating:   0%|          | 0/1 [00:00<?, ?window/s]INFO:p-1649965:t-139854902654784:timesfm_2p5_torch.py:_from_pretrained:Downloading checkpoint from Hugging Face repo google/timesfm-2.5-200m-pytorch\n",
      "INFO:p-1649965:t-139854902654784:timesfm_2p5_torch.py:_from_pretrained:Loading checkpoint from: /u/mgee2/.cache/huggingface/hub/models--google--timesfm-2.5-200m-pytorch/snapshots/1d952420fba87f3c6dee4f240de0f1a0fbc790e3/model.safetensors\n",
      "INFO:p-1649965:t-139854902654784:timesfm_2p5_torch.py:compile:When compiling, max horizon needs to be multiple of the output patch size 128. Using max horizon = 128 instead.\n",
      "100%|██████████| 5/5 [00:16<00:00,  3.33s/it]\n",
      "[_TimesFMV2_p5] Cross-validating: 100%|██████████| 1/1 [00:20<00:00, 20.06s/window]\n",
      "INFO:p-1649965:t-139854902654784:pandas.py:from_long_dataframe:Indexing data by 'ds'.\n",
      "INFO:p-1649965:t-139854902654784:pandas.py:from_long_dataframe:Grouping data by 'unique_id'; this may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 4.735700420273391\n",
      "            Iterations: 9\n",
      "            Function evaluations: 64\n",
      "            Gradient evaluations: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [00:00, 231.04it/s]\n",
      "/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 5/5 [00:00<00:00,  6.90it/s]\n",
      "100%|██████████| 5/5 [00:29<00:00,  5.86s/it]\n",
      "/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 5/5 [00:18<00:00,  3.61s/it]\n",
      "/u/mgee2/.conda/envs/gift/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:p-1649965:t-139854902654784:modeling_flowstate.py:__init__:Number of encoder parameters: 7885.8240000000005k\n",
      "INFO:p-1649965:t-139854902654784:modeling_flowstate.py:__init__:Number of dencoder parameters: 1181.952k (14.99%)\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.40it/s]\n",
      "INFO:p-1649965:t-139854902654784:timesfm_2p5_torch.py:_from_pretrained:Downloading checkpoint from Hugging Face repo google/timesfm-2.5-200m-pytorch\n",
      "INFO:p-1649965:t-139854902654784:timesfm_2p5_torch.py:_from_pretrained:Loading checkpoint from: /u/mgee2/.cache/huggingface/hub/models--google--timesfm-2.5-200m-pytorch/snapshots/1d952420fba87f3c6dee4f240de0f1a0fbc790e3/model.safetensors\n",
      "INFO:p-1649965:t-139854902654784:timesfm_2p5_torch.py:compile:When compiling, max horizon needs to be multiple of the output patch size 128. Using max horizon = 128 instead.\n",
      "100%|██████████| 5/5 [00:16<00:00,  3.34s/it]\n",
      "140it [00:00, 289.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett1/H have been written to ../results/tsorchestra/all_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_1649965/1082827166.py:121: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"MSE[mean]\"][0],\n",
      "/tmp/ipykernel_1649965/1082827166.py:122: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"MSE[0.5]\"][0],\n",
      "/tmp/ipykernel_1649965/1082827166.py:123: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"MAE[0.5]\"][0],\n",
      "/tmp/ipykernel_1649965/1082827166.py:124: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"MASE[0.5]\"][0],\n",
      "/tmp/ipykernel_1649965/1082827166.py:125: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"MAPE[0.5]\"][0],\n",
      "/tmp/ipykernel_1649965/1082827166.py:126: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"sMAPE[0.5]\"][0],\n",
      "/tmp/ipykernel_1649965/1082827166.py:127: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"MSIS\"][0],\n",
      "/tmp/ipykernel_1649965/1082827166.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"RMSE[mean]\"][0],\n",
      "/tmp/ipykernel_1649965/1082827166.py:129: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"NRMSE[mean]\"][0],\n",
      "/tmp/ipykernel_1649965/1082827166.py:130: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"ND[0.5]\"][0],\n",
      "/tmp/ipykernel_1649965/1082827166.py:131: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"mean_weighted_sum_quantile_loss\"][0],\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "from gift_eval.data import Dataset\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "from pytorch_lightning import seed_everything\n",
    "from tsorchestra.models.common.gluonts_predictor import GluonTSPredictor\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed_everything(42, workers=True, verbose=True)\n",
    "\n",
    "\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "\n",
    "model_name = \"tsorchestra\"\n",
    "\n",
    "# set the output directory and CSV file path\n",
    "output_dir = f\"../results/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")\n",
    "\n",
    "completed_datasets = set()\n",
    "# 1. Check if the results file exists and read the completed datasets to allow resuming\n",
    "if os.path.exists(csv_file_path):\n",
    "    print(f\"'{csv_file_path}' exists. Reading completed datasets...\")\n",
    "    with open(csv_file_path, \"r\", newline=\"\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                completed_datasets.add(row[0])\n",
    "    print(f\"Found {len(completed_datasets)} completed datasets.\")\n",
    "\n",
    "# 2. If the file doesn't exist, create it and write the header\n",
    "else:\n",
    "    with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write the header\n",
    "        writer.writerow(\n",
    "            [\n",
    "                \"dataset\",\n",
    "                \"model\",\n",
    "                \"eval_metrics/MSE[mean]\",\n",
    "                \"eval_metrics/MSE[0.5]\",\n",
    "                \"eval_metrics/MAE[0.5]\",\n",
    "                \"eval_metrics/MASE[0.5]\",\n",
    "                \"eval_metrics/MAPE[0.5]\",\n",
    "                \"eval_metrics/sMAPE[0.5]\",\n",
    "                \"eval_metrics/MSIS\",\n",
    "                \"eval_metrics/RMSE[mean]\",\n",
    "                \"eval_metrics/NRMSE[mean]\",\n",
    "                \"eval_metrics/ND[0.5]\",\n",
    "                \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "                \"domain\",\n",
    "                \"num_variates\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "    ds_key = ds_name.split(\"/\")[0]\n",
    "    print(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (\n",
    "            term == \"medium\" or term == \"long\"\n",
    "        ) and ds_name not in med_long_datasets.split():\n",
    "            continue\n",
    "\n",
    "        if \"/\" in ds_name:\n",
    "            ds_key = ds_name.split(\"/\")[0]\n",
    "            ds_freq = ds_name.split(\"/\")[1]\n",
    "            ds_key = ds_key.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "        else:\n",
    "            ds_key = ds_name.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "        ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "        if ds_config in completed_datasets:\n",
    "            print(f\"Skipping already completed dataset: {ds_config}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize the dataset\n",
    "        to_univariate = (\n",
    "            False\n",
    "            if Dataset(name=ds_name, term=term, to_univariate=False).target_dim == 1\n",
    "            else True\n",
    "        )\n",
    "        dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "        season_length = get_seasonality(dataset.freq)\n",
    "        print(f\"Dataset size: {len(dataset.test_data)}\")\n",
    "\n",
    "        predictor = GluonTSPredictor(forecaster=forecaster)\n",
    "\n",
    "        # Measure the time taken for evaluation\n",
    "        res = evaluate_model(\n",
    "            predictor,\n",
    "            test_data=dataset.test_data,\n",
    "            metrics=metrics,\n",
    "            axis=None,\n",
    "            mask_invalid_label=True,\n",
    "            allow_nan_forecast=False,\n",
    "            seasonality=season_length,\n",
    "        )\n",
    "\n",
    "        # Append the results to the CSV file\n",
    "        with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    ds_config,\n",
    "                    model_name,\n",
    "                    res[\"MSE[mean]\"][0],\n",
    "                    res[\"MSE[0.5]\"][0],\n",
    "                    res[\"MAE[0.5]\"][0],\n",
    "                    res[\"MASE[0.5]\"][0],\n",
    "                    res[\"MAPE[0.5]\"][0],\n",
    "                    res[\"sMAPE[0.5]\"][0],\n",
    "                    res[\"MSIS\"][0],\n",
    "                    res[\"RMSE[mean]\"][0],\n",
    "                    res[\"NRMSE[mean]\"][0],\n",
    "                    res[\"ND[0.5]\"][0],\n",
    "                    res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                    dataset_properties_map[ds_key][\"domain\"],\n",
    "                    dataset_properties_map[ds_key][\"num_variates\"],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        print(f\"Results for {ds_name} have been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>eval_metrics/MSE[mean]</th>\n",
       "      <th>eval_metrics/MSE[0.5]</th>\n",
       "      <th>eval_metrics/MAE[0.5]</th>\n",
       "      <th>eval_metrics/MASE[0.5]</th>\n",
       "      <th>eval_metrics/MAPE[0.5]</th>\n",
       "      <th>eval_metrics/sMAPE[0.5]</th>\n",
       "      <th>eval_metrics/MSIS</th>\n",
       "      <th>eval_metrics/RMSE[mean]</th>\n",
       "      <th>eval_metrics/NRMSE[mean]</th>\n",
       "      <th>eval_metrics/ND[0.5]</th>\n",
       "      <th>eval_metrics/mean_weighted_sum_quantile_loss</th>\n",
       "      <th>domain</th>\n",
       "      <th>num_variates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ett1/H/short</td>\n",
       "      <td>tsorchestra</td>\n",
       "      <td>92.422526</td>\n",
       "      <td>92.422526</td>\n",
       "      <td>4.757038</td>\n",
       "      <td>0.792472</td>\n",
       "      <td>0.45415</td>\n",
       "      <td>0.250998</td>\n",
       "      <td>5.452635</td>\n",
       "      <td>9.613664</td>\n",
       "      <td>0.448709</td>\n",
       "      <td>0.22203</td>\n",
       "      <td>0.170857</td>\n",
       "      <td>Energy</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset        model  eval_metrics/MSE[mean]  eval_metrics/MSE[0.5]  \\\n",
       "0  ett1/H/short  tsorchestra               92.422526              92.422526   \n",
       "\n",
       "   eval_metrics/MAE[0.5]  eval_metrics/MASE[0.5]  eval_metrics/MAPE[0.5]  \\\n",
       "0               4.757038                0.792472                 0.45415   \n",
       "\n",
       "   eval_metrics/sMAPE[0.5]  eval_metrics/MSIS  eval_metrics/RMSE[mean]  \\\n",
       "0                 0.250998           5.452635                 9.613664   \n",
       "\n",
       "   eval_metrics/NRMSE[mean]  eval_metrics/ND[0.5]  \\\n",
       "0                  0.448709               0.22203   \n",
       "\n",
       "   eval_metrics/mean_weighted_sum_quantile_loss  domain  num_variates  \n",
       "0                                      0.170857  Energy             7  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"../results/{model_name}/all_results.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gift",
   "language": "python",
   "name": "gift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
